"""
QuSpin .lvm Reader for MNE-Python

This module provides an MNE-Python I/O plugin for reading data recorded by QuSpin
magnetometers stored in `.lvm` format. It supports both preloaded and on-demand
data reading, integrating seamlessly with MNE workflows.

Main Features:
- Reads QuSpin .lvm files into `mne.io.Raw` objects.
- Supports metadata such as channel types, units, and sampling frequency.
- Provides both preloaded and memory-efficient on-demand reading.

Example
-------
>>> from mne.io.quspin import read_raw_quspin_lvm
<<<<<<< HEAD
>>> raw = read_raw_quspin_lvm('mne/io/quspin/tests/data/quspin_N1_test_data.lvm', preload=True, verbose = 'error')
=======
>>> raw = read_raw_quspin_lvm('example.lvm', preload=True)
>>> print(raw.info)
>>>>>>> 1e81ad98a3451ffac893f66905d0c024a769a514
"""

from datetime import datetime, timezone

import numpy as np

from mne import create_info
from mne.io import BaseRaw
from mne.io.constants import FIFF
from mne.utils import logger, verbose


def read_raw_quspin_lvm(fname, preload=False, verbose=None):
    """
    Read QuSpin .lvm data as an MNE Raw object.

    Parameters
    ----------
    fname : str
        Path to the `.lvm` file.
    preload : bool
        If True, load all data into memory for fast access. If False, data is read
        on-demand, which is more memory-efficient for large files.
    verbose : bool | str | None
        Verbosity level for logging. If None, defaults to the MNE global verbosity level.

    Returns
    -------
    raw : RawQuSpin
        An instance of `RawQuSpin` containing the parsed data.

    Notes
    -----
    - For large `.lvm` files, consider setting `preload=False` to reduce memory usage.
    - Ensure that the `.lvm` file contains properly formatted metadata (e.g., sampling
      frequency and channel units) for accurate processing.

    Example
    -------
    >>> from mne.io.quspin import read_raw_quspin_lvm
    <<<<<<< HEAD
    >>> raw = read_raw_quspin_lvm('mne/io/quspin/tests/data/quspin_N1_test_data.lvm', preload=True, verbose = 'error')
    =======
    >>> raw = read_raw_quspin_lvm('example.lvm', preload=True)
    >>> print(raw.info)
    >>>>>>> 1e81ad98a3451ffac893f66905d0c024a769a514
    """
    return RawQuSpin(fname=fname, preload=preload, verbose=verbose)


class RawQuSpin(BaseRaw):
    """
    Raw object for QuSpin .lvm data.

    This class inherits from `mne.io.BaseRaw` and provides functionality for reading
    `.lvm` files generated by QuSpin systems. It supports both preloaded and on-demand
    data access, making it memory-efficient for large datasets.

    Parameters
    ----------
    fname : str
        Path to the `.lvm` file.
    preload : bool
        If True, load all data into memory for fast access. If False, data is read
        on-demand.
    verbose : bool | str | None
        Verbosity level for logging.

    Notes
    -----
    - The `.lvm` file must include properly formatted headers, including channel names,
    sampling frequency, and units.
    - If preload is False, data is read from disk each time a segment is accessed.
    """

    @verbose
    def __init__(self, fname, preload=False, verbose=None):
        self.fname = fname

        # ---------------------------------------------------------
        # 1) Parse the file header and optionally load the data
        # ---------------------------------------------------------
        (section_arrays, col_names, raw_data, data_start_line, n_data_samples) = (
            self._read_lvm_header(fname=fname, preload=preload)
        )

        # ---------------------------------------------------------
        # 2) Create MNE Info
        # ---------------------------------------------------------
        info = self._create_info(section_arrays, col_names)

        # ---------------------------------------------------------
        # 3) Preload logic
        # ---------------------------------------------------------
        # raw_data: shape (n_samples, n_channels) if preload=True, else None
        if preload and raw_data is not None:
            # Transpose to MNE shape (n_channels, n_samples)
            self._data = raw_data.T
        else:
            self._data = None

        self._data_start_line = data_start_line
        self._n_samples = n_data_samples
        self._n_channels = len(col_names)

        # ---------------------------------------------------------
        # 4) Provide first/last_samps as single-element tuples
        #    to satisfy older MNE versions' requirements
        # ---------------------------------------------------------
        # We always define them, regardless of preload being array or bool.
        # This avoids the TypeError: "object of type 'NoneType' has no len()"
        # that occurs if we pass None.
        first_samp = 0
        last_samp = max(0, n_data_samples - 1)  # avoid negative if no samples

        first_samps = (first_samp,)
        last_samps = (last_samp,)

        # For older MNE, if we pass an ndarray to `preload`, we typically
        # wouldn't need first_samps, but some checks still expect them anyway.
        # So we provide them unconditionally.
        if isinstance(self._data, np.ndarray):
            preload_arg = self._data  # a NumPy array
        else:
            preload_arg = False

        # ---------------------------------------------------------
        # 5) Initialize BaseRaw
        # ---------------------------------------------------------
        super().__init__(
            info=info,
            preload=preload_arg,
            first_samps=first_samps,
            last_samps=last_samps,
            verbose=verbose,
        )

    @verbose
    def _read_lvm_header(self, fname, preload, verbose=None):
        """
        Parse the .lvm file header and optionally read all numeric data.

        Parameters
        ----------
        fname : str
            Path to the `.lvm` file.
        preload : bool
            If True, read all numeric data into memory. If False, only the header is parsed.
        verbose : bool | str | None
            Verbosity level for logging.

        Returns
        -------
        section_arrays : dict
            A dictionary containing parsed metadata from the file header. Keys include:
            "Samples", "Date", "Time", "Y_Unit_Label", "X_Dimension", "X0", and "Delta_X".
        col_names : list of str
            Channel names parsed from the file header.
        raw_data : np.ndarray | None
            If `preload=True`, a NumPy array of shape (n_samples, n_channels) containing
            the raw data. Otherwise, None.
        data_start_line : int
            Line index in the file where numeric data begins.
        n_data_samples : int
            Number of data samples available in the file.

        Raises
        ------
        ValueError
            If the file header is improperly formatted or required metadata is missing.

        Notes
        -----
        - The final column of the `.lvm` file (comment column) is excluded from both
        `col_names` and `raw_data`.
        """
        section_labels = [
            "Samples",
            "Date",
            "Time",
            "Y_Unit_Label",
            "X_Dimension",
            "X0",
            "Delta_X",
        ]
        section_arrays = {label: [] for label in section_labels}
        col_names = []
        raw_data = None

        HEADER_END_LINE = 22  # line containing channel names
        DATA_START_LINE = 23  # numeric data starts here

        with open(fname) as f:
            lines = f.readlines()

        total_lines = len(lines)

        # --------------------------------------------------------
        # 1) Parse the header lines (0..22)
        # --------------------------------------------------------
        for i, line in enumerate(lines[: HEADER_END_LINE + 1]):
            line = line.strip()

            # 1a) Parse metadata lines
            if i < HEADER_END_LINE:
                for label in section_labels:
                    if line.startswith(label):
                        tokens = line.split("\t")[1:]
                        section_arrays[label] = tokens

            # 1b) Parse the line with channel names (i == HEADER_END_LINE)
            elif i == HEADER_END_LINE:
                tokens = line.split("\t")
                # Remove trailing empty tokens (if there's an extra tab at the end)
                while tokens and not tokens[-1].strip():
                    tokens.pop()

                # Skip the first token (often "Channels"), skip the last token ("comment")
                # So the channel names are in tokens[1:-1].
                col_names = tokens[1:-1]

        # --------------------------------------------------------
        # 2) Determine number of data lines
        # --------------------------------------------------------
        n_data_lines = max(0, total_lines - DATA_START_LINE)

        # Log some basic info about the data loading
        logger.info(
            f"Read {total_lines} lines from '{fname}'. Header lines: {HEADER_END_LINE + 1}, "
            f"data lines: {n_data_lines}, preload={preload}."
        )
        logger.debug(f"Parsed channel names: {col_names}")

        # --------------------------------------------------------
        # 3) If preload=True, read all numeric data (excl. 'comment' column)
        # --------------------------------------------------------
        if preload and n_data_lines > 0:
            data_list = []
            expected_len = 1 + len(col_names)  # first token + channels
            for line in lines[DATA_START_LINE:]:
                vals = line.strip().split("\t")

                # Remove trailing empty tokens
                while vals and not vals[-1].strip():
                    vals.pop()

                # Ensure at least (time/index + #channels) columns are present
                if len(vals) < expected_len:
                    raise ValueError(
                        f"Data line {i} has only {len(vals)} columns; "
                        f"expected at least {expected_len} (time + {len(col_names)} channels). "
                        f"Line content: {vals}"
                    )

                # Parse exactly the channel columns (skipping the first token since that is the time value)
                float_vals = [float(v) for v in vals[1 : 1 + len(col_names)]]
                data_list.append(float_vals)

            raw_data = np.array(data_list, dtype=float)

        return section_arrays, col_names, raw_data, DATA_START_LINE, n_data_lines

    @verbose
    def _create_info(self, section_arrays, channel_names, verbose=None):
        """
        Create the MNE Info object from parsed header metadata.

        Parameters
        ----------
        section_arrays : dict
            Dictionary of header metadata parsed from the `.lvm` file. Must include:
            "Delta_X" (sampling interval), "Y_Unit_Label" (units per channel), and
            "Date" / "Time" (optional).
        channel_names : list of str
            Channel names parsed from the `.lvm` file.
        verbose : bool | str | None
            Verbosity level for logging.

        Returns
        -------
        info : mne.Info
            The MNE Info object containing channel information, sampling rate, and
            measurement date.

        Notes
        -----
        - Units are assigned to channels based on `Y_Unit_Label` in `section_arrays`.
        Supported units are:
            - 'pT' -> Tesla (1e-12 multiplier)
            - 'V'  -> Volts
            - '1|0' -> Unitless (digital trigger)
        - Raises a `ValueError` if the number of `Y_Unit_Label` entries does not match
        the number of channels.
        """
        # 1) Determine sampling frequency from Delta_X
        try:
            delta_x_str = section_arrays.get("Delta_X", [None])[0]
            if delta_x_str is None:
                raise ValueError
            delta_x = float(delta_x_str)
            sfreq = round(1.0 / delta_x, 6)
        except (ValueError, IndexError, TypeError):
            raise ValueError("Delta_X value is missing or invalid in the .lvm file.")

        # 2) Infer channel types
        channel_types = []
        for ch_name in channel_names:
            if ch_name.startswith(("X", "Y", "Z")):
                channel_types.append("mag")  # magnetometer
            elif ch_name.startswith("T"):
                channel_types.append("stim")  # digital trigger
            elif ch_name.startswith("A"):
                channel_types.append("stim")  # analog trigger
            else:
                channel_types.append("misc")

        # 3) Parse measurement date/time
        date_str = section_arrays.get("Date", [None])[0]
        time_str = section_arrays.get("Time", [None])[0]

        meas_date = None
        if date_str is not None:
            try:
                date_str = date_str.replace("/", "-")
                if time_str:
                    time_str = time_str.split(".")[0]  # drop fractional secs
                    meas_date = datetime.strptime(
                        f"{date_str} {time_str}", "%Y-%m-%d %H:%M:%S"
                    ).replace(tzinfo=timezone.utc)
                else:
                    meas_date = datetime.strptime(date_str, "%Y-%m-%d").replace(
                        tzinfo=timezone.utc
                    )
            except ValueError:
                pass  # leave meas_date as None if parsing fails

        # Create the info object
        info = create_info(ch_names=channel_names, sfreq=sfreq, ch_types=channel_types)
        info.set_meas_date(meas_date)

        # 4) Assign units
        y_labels = section_arrays.get(
            "Y_Unit_Label", []
        )  # get the labels from the lvm file

        # Check that there are enough labels for each channel.
        if len(y_labels) != len(info["chs"]):
            raise ValueError(
                f"Expected {len(info['chs'])} Y_Unit_Label entries, "
                f"but got {len(y_labels)}."
            )

        for y_unit_label, ch in zip(y_labels, info["chs"]):
            if y_unit_label == "pT":
                ch["unit"] = FIFF.FIFF_UNIT_T  # Tesla base unit
                ch["unit_mul"] = -12  # pT = 1e-12 T
            elif y_unit_label == "V":
                ch["unit"] = FIFF.FIFF_UNIT_V  # Volts
                ch["unit_mul"] = 0
            elif y_unit_label == "1|0":
                ch["unit"] = FIFF.FIFF_UNIT_NONE
                ch["unit_mul"] = 0
            else:
                # Unrecognized or missing label => default to unitless
                ch["unit"] = FIFF.FIFF_UNIT_NONE
                ch["unit_mul"] = 0

        return info

    @verbose
    def _read_segment_file(self, data, idx, fi, start, stop, cals, mult, verbose=None):
        """Read a chunk of data from a QuSpin .lvm file for on-demand access.

        This method is inspired by the example from another company but tailored
        for QuSpin .lvm data. It handles partial reads by skipping 'start' lines,
        reading the required samples, and applying calibration factors. It ignores
        any 'projector' argument.

        Parameters
        ----------
        data : np.ndarray
            Array to store the requested data in-place. Shape:
            (len(idx), stop - start).
        idx : list of int
            Channel indices to read and place into 'data'.
        fi : int
            File index (if multiple files are used). For a single .lvm file, this
            can be 0 or ignored.
        start : int
            First sample index to read from the file.
        stop : int
            One past the last sample index to read.
        cals : np.ndarray
            Calibration factors for each channel (shape: n_channels,).
        mult : np.ndarray | None
            Additional multiplication matrix. If present, typically shape:
            (len(idx), len(idx)) or similar.

        Notes
        -----
        1. We assume there's only one file to read (fi=0).
        2. This code ignores any 'projector' approach.
        3. The file is read line-by-line, skipping the 'start' lines first,
        then reading (stop - start) lines until end of file.
        """
        # If the data is already in memory (preloaded), just slice:
        if self._data is not None:
            # Logging some infor about data loading
            logger.info(
                f"Reading segment from preloaded data: start={start}, stop={stop}, "
                f"channels requested={len(idx)}."
            )
            data[...] = self._data[idx, start:stop] * cals[idx, np.newaxis]
            if mult is not None:
                data[...] = mult @ data
            return

        # -- Otherwise, we parse from disk line-by-line.
        # Log some basic info
        logger.info(
            f"Reading segment on demand from file: start={start}, stop={stop}, "
            f"channels requested={len(idx)}."
        )

        # Calculate how many samples we want to read
        n_samps = stop - start
        # Prepare a temporary buffer of shape (n_channels, n_samps)
        block_data = np.zeros((self._n_channels, n_samps), dtype=float)

        # Open the file
        with open(self.fname) as fid:
            # --- Safely skip lines up to self._data_start_line + start ---
            lines_to_skip = self._data_start_line + start
            for i in range(lines_to_skip):
                skip_line = fid.readline()
                # If we've reached EOF before skipping all lines, stop.
                if not skip_line:
                    # All subsequent data will stay zero if we have no lines to read
                    logger.warning(
                        f"Requested start={start} exceeds file length. "
                        f"Filling all {n_samps} samples with zeros."
                    )
                    break

            # Read exactly (stop - start) lines, or until EOF
            for i in range(n_samps):
                line = fid.readline()
                if not line:
                    # End of file: warn the user & leave the remainder as zeros
                    logger.warning(
                        f"Requested samples exceed available data. "
                        f"Filling remaining {n_samps - i} samples with zeros."
                    )
                    break

                # Process the line
                line = line.strip()
                vals = line.split("\t")  # Split by tabs
                floats = [
                    float(v) for v in vals[1:]
                ]  # skip the first token,  the remaining columns are floats (mag, stim, etc.)
                block_data[:, i] = floats

        # -- Apply calibration for all channels
        # Expand cals so it can multiply block_data channel-wise
        block_data *= cals[:, np.newaxis]

        # -- Now pick only the requested channels
        block_data = block_data[idx, :]

        # -- If there's an additional multiplier (e.g., an MNE projection step):
        if mult is not None:
            block_data = mult @ block_data

        # -- Finally, store into 'data' in-place
        data[...] = block_data
